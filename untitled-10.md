# Untitled

## **HYPER PARAMETER OPTIMIZATION**

[**1. Using HyperOpt**](http://hyperopt.github.io/hyperopt/) **-** 

* **Random Search**
* **Tree of Parzen Estimators \(TPE\)**

**Hyperopt has been designed to accommodate Bayesian optimization algorithms based on Gaussian processes and regression trees, but these are not currently implemented.**

**All algorithms can be run either serially, or in parallel by communicating via** [**MongoDB**](http://www.mongodb.org/)**.**

* [**Mlflow, Hyperparameterhunter,hyperopt, concept drift, unit tests.**](https://towardsdatascience.com/putting-ml-in-production-ii-logging-and-monitoring-algorithms-91f174044e4e)
* [**Hyperopt**](http://hyperopt.github.io/hyperopt/) **for hyper parameter search**

**2.** [**HyperparameterHunter**](https://github.com/HunterMcGushion/hyperparameter_hunter) **-   
 provides a wrapper for machine learning algorithms that saves all the important data. Simplify the experimentation and hyperparameter tuning process by letting HyperparameterHunter do the hard work of recording, organizing, and learning from your tests â€” all while using the same libraries you already do. Don't let any of your experiments go to waste, and start doing hyperparameter optimization the way it was meant to be.3.** [**Implementation and comparison**](https://towardsdatascience.com/putting-ml-in-production-ii-logging-and-monitoring-algorithms-91f174044e4e) **- HH slower than HO due to usage of skopt.**

